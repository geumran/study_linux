The first kpatch submission
By Jonathan Corbet
May 7, 2014

북반구에 봄이 왔다, 따라서 젊은 커널 개발자들의 생각은 자연스레 동적 커널 패칭으로 옮겨갈 것이다. 지난 주에 SUSE의 kGraft 라이브패칭 메카니즘이 포스팅된 것을 보았다. 그 직후, 레드햇의 개발자들은 이와 경쟁메카니즘인 kpatch을 들고 나왔다. 두 그룹이 취한 접근방식은 몇가지 흥미로운 유사점을 보여주지만, 중요한 차이점도 존재한다.
It is spring in the northern hemisphere, so a young kernel developer's thoughts naturally turn to … dynamic kernel patching. Last week saw the posting of SUSE's kGraft live-patching mechanism; shortly thereafter, developers at Red Hat came forward with their competing kpatch mechanism. The approaches taken by the two groups show some interesting similarities, but also some significant differences.

kGraft와 마찬가지로, kpatch는 실행중인 커널의 전체 함수를 교체한다. 커널 패치는 어떤 함수가 변경되었는지 결정하기 위해 kpatch 툴(패치에 포함된 것은 아니지만 this repository에서 구할 수 있다)에 의해 처리되어 변경된 함수의 새로운 버전을 담고 있는 loadable 커널 모듈을 만드는데 사용할 정보로 쓰인다.
Like kGraft, kpatch replaces entire functions within a running kernel. A kernel patch is processed to determine which functions it changes; the kpatch tools (not included with the patch, but available in this repository) then use that information to create a loadable kernel module containing the new versions of the changed functions.

이전 버전의 함수 호출을 가로채서 최신 버전으로 컨트롤을 리다이렉트  위해 kpatch  core 코드안의 kpatch_register()를 호출하여 ftrace의 function tracing 메카니즘을 사용할 것이다. 지금까지는, kGraft 와 비슷하게 들리겠지만, 세부사항을 살펴보면 다른 점이 보인다.
A call to the new kpatch_register() function within the core kpatch code will use the ftrace function tracing mechanism to intercept calls to the old functions, redirecting control to the new versions instead. So far, it sounds a lot like kGraft, but that resemblance fades a bit once one looks at the details.

kGraft는 교체된 함수의 새버전과 이전버전이 모두 커널에서 활성화된 상태로 유지되는 복잡한 상황을 갖는다. 그 이유는 각각의 실행중인 프로세스가 (hopefull) 안전한 시점에 새로운 우주로 전환하는 것을 허용하기 위해서다.
KGraft goes through a complex dance during which both the old and new versions of a replaced function are active in the kernel; this is done in order to allow each running process to transition to the "new universe" at a (hopefully) safe time.

kpatch는 비교적 덜 복잡하다. stop_machine()을 호출하는 것을 시작으로 모든 시스템의 cpu가 정지시킨다. 그런 다음 kpatch는 커널 모드에서 실행중인 모든 프로세스의 스택을 검사하여 해당 함수에서 실행중인 프로세스가 없는 것을 확인한다. 패치된 함수 중 하나가 실행중이라면 패치를 적용하는 과정은 실패할 것이다. 만약 상황이 정상이라면 kpatch는 이전 함수를 없앤다(혹은 더욱 정확하게 말하자면, ftrace 핸들러가 이전함수를 건너서 최신함수를 지정하도록 한다). 프로세스들이 이전 혹은 최신 상황에 위치해있는지에 대한 추적은 할 수가 없다. 대신에 가능하다면 모두가 새로운 상황을 맞이하도록 강제한다.
Kpatch is rather less subtle: it starts by calling stop_machine() to bring all other CPUs in the system to a halt. Then, kpatch examines the stack of every process running in kernel mode to ensure that none are running in the affected function(s); should one of the patched functions be active, the patch-application process will fail. If things are OK, instead, kpatch patches out the old functions completely (or, more precisely, it leaves an ftrace handler in place that routes around the old function). There is no tracking of whether processes are in the "old" or "new" universe; instead, everybody is forced to the new universe immediately if it is possible.

이런 접근법에는 몇가지 단점이 있다. stop_machine()은 kpatch의 massive sledgehammer이다. 커널개발자는 가능하다면 해당 함수의 사용을 피하고자 한다. 만약 커널 코드가 패치할 대상인 함수중 하나를 실행중이라면, kpatch는 실패할 것이다. 대신에 kGraft는 한번에 한 프로세스씩 새로운 기능으로 시스템을 천천히 패치한다. 어떤 함수(예를 들어 schedule(), do_wait(), irq_thread())는 항상 커널 어딘가에서 실행되므로 kpatch는 해당 함수들을 패치하는데 사용할 수 없다. 일반적인 시스템에서, 이런 방식의 라이브패치가 블러킹될 수 있는 수십가지 함수들이 존재할 것이다. 이는 커널의 수천가지 함수중에 아주 작은 부분 집합이다.
There are some downsides to this approach. stop_machine() is a massive sledgehammer of a tool; kernel developers prefer to avoid it if at all possible. If kernel code is running inside one of the target functions, kpatch will simply fail; kGraft, instead, will work to slowly patch the system over to the new function, one process at a time. Some functions (examples would include schedule(), do_wait(), or irq_thread()) are always running somewhere in the kernel, so kpatch cannot be used to apply a patch that modifies them. On a typical system, there will probably be a few dozen functions that can block a live patch in this way — a pretty small subset of the thousands of functions in the kernel.

stop_machine()을 사용하는 kpatch가 다소 무겁게 처리하는 것처럼 보일 수 있지만, 초기에는 더 강한 접근법을 가져가야 한다는 개발자도 있었다. Ingo Molnar는 커널안에서 동작중인 프로세스가 없는 것을 보장하기 위해  프로세스 프리저를 사용해야 한다고 제안했다(일반적으로 시스템을 하이버네이션 시킬때 사용함) . 이런 방법은 라이브 커널 패칭을 좀 더 느리게 만들 수는 있지만 말이다.
While kpatch, with its use of stop_machine(), may seem heavy-handed, there are some developers who would like to see it take an even stronger approach initially: Ingo Molnar suggested that it should use the process freezer (normally used when hibernating the system) to be absolutely sure that no processes have any running state within the kernel. That would slow live kernel patching even more, but, as he put it:

만약 배포판들이 라이브패칭을 사용하는 방향으로 간다면(현재 그렇게 되고 있다!), 실질적으로 이런 무서운 방법???? 이를테면 이 오히려 내게는 더 필요하다.
Well, if distros are moving towards live patching (and they are!), then it looks rather necessary to me that something scary as flipping out live kernel instructions with substantially different code should be as safe as possible, and only then fast.


kpatch 개발자 Josh Poimboeuf가 지적한것처럼 이 접근 방식의 문제점은 unfreezable한 커널 스레드가 많다는 것이다. Frederic Weisbecker는 대신 커널 스레드 파킹 메카니즘을 사용할 수 있다고 제안했다. 어쨌건 Ingo는 라이브 패칭을 막는 커널 스레드는 in short order로 수정될 것이라는 생각을 냈다. 커널 스레드를 freezing 혹은 parking하는 것이 정말로 피룡한지에 대한 합의는 없었지만, 느리지만 안전한 방식의 방향으로 의견이 기울었고 성능을 향상시키는 것은 나중으로 미뤄진것처럼 보였다.
The hitch with this approach, as noted by kpatch developer Josh Poimboeuf, is that there are a lot of unfreezable kernel threads. Frederic Weisbecker suggested that the kernel thread parking mechanism could be used instead. Either way, Ingo thought, kernel threads that prevented live patching would be likely to be fixed in short order. There was not a consensus in the end on whether freezing or parking kernel threads was truly necessary, but opinion did appear to be leaning in the direction of being slow and safe early on, then improving performance later.

다른 문제는 커널 안의 데이터의 해석이나 형식을 변경하는 패치에 대한 것이다. kGraft는 "univers" 메카니즘으로 간단한 경우를 핸들링하려고 하지만, 많은 경우에 더 복잡한 방식을 필요로 할 것이다. kGraft 개발자 JIri Kosina에 따르면 모든 프로세스가 새 코드로 변환될 때까지 변경된 데이터 구조의 두 형태를 이해하기 위해 쓰이는 "band-aid 함수"를 사용하는 메커니즘이 있다. 전환이 완료된 후에 변경된 데이터 구조의 이전 버전을 write하는 코드는 패치할 수 있지만 다음 재부팅 때까지 이전 데이터 구조를 읽는 코드를 유지해야 할 수도 있다.
The other question that has come up has to do with patches that change the format or interpretation of in-kernel data. KGraft tries to handle simple cases with its "universe" mechanism but, in many situations, something more complex will be required. According to kGraft developer Jiri Kosina, there is a mechanism in place to use a "band-aid function" that understands both forms of a changed data structure until all processes have been converted to the new code. After that transition has been made, the code that writes the older version of the changed data structure can be patched out, though it may be necessary to retain code that reads older data structures until the next reboot.

kpatch에서는 현재 데이터 구조를 변경하는 provision이 없다. 가까운 미래를 위한 계획은 라이브 패치와 함께 콜백을 추가해서 패키징하는 것이 있다. 그 작업은 시스템이 정지되고 패치가 적용되는 동안 영향을 받는 모든 데이터 구조를 검색하고 변환하는 것이다. 이 접근법은 오래된 데이터 구조에 대처할 수 있는 능력을 유지할 필요없이 모든 영향을 받는 구조가 패칭시간에만 존재할 것이다. 대부분의 경우 a tall order이다.
On the kpatch side, instead, there is currently no provision for making changes to data structures at all. The plan for the near future is to add a callback that can be packaged with a live patch; its job would be to search out and convert all affected data structures while the system is stopped and the patch is being applied. This approach has the potential to work without the need for maintaining the ability to cope with older data structures, but only if all of the affected structures can be located at patching time — a tall order, in many cases.

좋은 소식은 Jiri가 말한것처럼 소수의 패치만 커널 자료구조를 변경하게 한다는 것이다.(라이브 패칭을 고려하게 하는 타입의 패치를 말함)
The good news is that few patches (of the type that one would consider for live patching) make changes to kernel data structures. As Jiri put it:

우리는 매우 가벼운 예비분석을 수행했고 적절한 다운타임을 위한 충분한 시간이 없는 hot/live 패치로 보내는게 적절한했다.(예를 들어 CVE 심각도가 매우 높을때(local root), 등등) 대부분의 경우에, 한 줄 또는 몇줄짜리 패치이고, 추가 체크를 추가하거나 경계를 수정하곤 한다. 추가 처리가 필요로 하는 경우는 몇년동안 단 1-2건 정도이다. 
We've done some very light preparatory analysis and went through patches which would make most sense to be shipped as hot/live patches without enough time for proper downtime scheduling (i.e. CVE severity high enough (local root), etc). Most of the time, these turn out to be a one-or-few liners, mostly adding extra check, fixing bounds, etc. There were just one or two in a few years history where some extra care would be needed.

따라서 어떻게 커널에서 동작중인 코드가 변경될지에 대한 질문의 답이 당장 필요한것과 달리 데이터 관련 변경 사항을 안전하게 처리하는 것에 대한 질문은 미뤄질 수 있다. 이미 이 주제가 2014년 8월의 커널 서밋에서 논의되어야 한다는 제안이 있었다. 이것은 가능한 얘기이지만, 그전에 관련된 개발자들 사이에서 approach를 합칠 방법을 찾고 통합해야 한다. 최종 목표에 대한 진정한 의견 차이는 없다. 단지 목표를 달성하기 위한 최선을 방법을 찾아야 할 뿐이다.
So the question of safely handling data-related changes can likely be deferred for now while the question how to change the code in a running kernel is answered. There have already been suggestions that this topic should be discussed at the 2014 Kernel Summit in August. It is entirely possible, though, that the developers involved will find a way to combine their approaches and get something merged before then. There is no real disagreement over the end goal, after all; it's just a matter of finding the best approach for the implementation of that goal.
